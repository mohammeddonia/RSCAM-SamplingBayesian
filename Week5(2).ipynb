{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSCAM Week 5 exercises\n",
    "\n",
    "_(Updated Friday February 15th 2019)_\n",
    "\n",
    "**[0] Please rename your submitted file in the format `firstname_lastname.ipynb` using _File->Rename_.**\n",
    "\n",
    "### The Rosenbrock problem\n",
    "\n",
    "An important test for any sampling application is how long it takes the algorithm to traverse barriers (such as in the previous weeks), but also how long it takes to explore the surrounding space. We can count barrier hops to test the former, but what about the latter?\n",
    "\n",
    "We can use the autocorrelation time to give us an idea of the decorrelation rate of the slgorithm. If it samples \"iid\" then the autocorrelation time is 1, because every sample is decorrelated from one another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be looking at the Rosenbrock _banana_ problem. In two dimensions $[x,y]=q\\in\\mathbb{R}^2$ we have $\\rho(x,y) \\propto \\exp(-U(x,y))$ as usual, where\n",
    "\n",
    "$$ U(x,y) =  (1-x)^2 + 4( 2y-x^2)^2 $$\n",
    "\n",
    "We plot the energy function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U(x,y):\n",
    "    \n",
    "    return (1-x)**2 + 4*(2*y-x**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = np.meshgrid( np.linspace(-3,5,200) , np.linspace(-1,11,100) )\n",
    "rho = U(X,Y)\n",
    "plt.pcolor(X,Y,rho,vmin=0,vmax=15)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('The Rosenbrock function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution defined by this energy function has a strong curvature that can make travelling around the space tricky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, we may use numerical methods to generate points distributed according to the distribution $\\rho$ however this may introduce a bias in the form of discretization error. We can use a Metropolis (or Metropolis-Hastings) test to correct for this error to remove the bias. We can even use these tests to transform any reversible stepping method to sample perfectly a given distribution (although it may not necessarily be efficient).\n",
    "\n",
    "The quintessential example of this is the Metropolis Random Walk scheme.\n",
    "\n",
    "$$ \\left| \\begin{array}{rcl} \n",
    "R_t&\\leftarrow&\\text{Normal}(0,1)\\\\\n",
    "S_t&\\leftarrow&\\text{Uniform}(0,1)\\\\\n",
    "\\qquad q^*_{t+1}&\\leftarrow&q_t + h\\, R_t\\\\\n",
    "\\text{If } S_t &<& \\rho(q^*_{t+1} )/\\rho(q_t) \\text{ then:}\\\\\n",
    "\\qquad \\textit{(accept)}\\\\\n",
    "\\qquad q_{t+1} &\\leftarrow& q^*_{t+1}\\\\\n",
    "\\text{otherwise:}\\\\\n",
    "\\qquad \\textit{(reject)}\\\\\n",
    "\\qquad q_{t+1} &\\leftarrow& q_t\\\\\n",
    "\\text{end if}\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "Note here that we don't need to know the normalization constant for $\\rho$, as we only care about the ratio between the $\\rho$ at the _proposed_ point $q^*$ and the _current_ point $q$.\n",
    "\n",
    "**[1] Code a version of the Metropolis algorithm below. It should take as input the current position $q$ and a stepsize $h$. It should output the new position $q$ and an integer ($1$ if new point was accepted, $0$ if new point was rejected). **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rho(q):\n",
    "    x = q[0]\n",
    "    y = q[1]\n",
    "    \n",
    "    return np.exp(-U(x,y))\n",
    "\n",
    "def Metropolis(q0,h):\n",
    "    \n",
    "    # Your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plug this algorithm into the `run_simulation` routine like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation( q0, Nsteps, h, step_function):\n",
    "    \n",
    "    \n",
    "    q_traj = [np.copy(q0)]  \n",
    "\n",
    "    q = np.copy(q0)  \n",
    "    total_accepted = 0\n",
    "\n",
    "    for n in range(Nsteps):\n",
    "        q, accepted = step_function(q, h) \n",
    "\n",
    "        q_traj += [q]  \n",
    "        total_accepted += accepted\n",
    "\n",
    "    q_traj = np.array(q_traj)   \n",
    "\n",
    "    return q_traj, total_accepted/Nsteps\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running is then easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = [1.0,1.0/2]\n",
    "Nsteps = 1000\n",
    "h = 0.25\n",
    "q_traj, acc = run_simulation( q0, Nsteps, h, Metropolis) \n",
    "print('Acceptance probability: ' + str(acc))\n",
    "\n",
    "X,Y = np.meshgrid( np.linspace(-3,5,200) , np.linspace(-1,11,100) )\n",
    "plt.pcolor(X,Y,U(X,Y),vmin=0,vmax=15)\n",
    "plt.plot(q_traj[:,0],q_traj[:,1],'r.')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('The Rosenbrock function')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The acceptance rate should be around $0.45$ here, so nearly half of the new points are accepted. It should be clear that increasing $h$ will make bigger moves in space, but the probability of them being accepted will decrease. Smaller $h$ will make smaller moves, but the acceptance probability will increase (_try this yourself if you wish_).\n",
    "\n",
    "This begs the question: what's the best choice for $h$? Do we want rare-but-large moves, or frequent-but-small moves? This is not an easy question to answer in general, but one way to check is to look at the autocorrelation time.\n",
    "\n",
    "Consider an infinitely long trajectory $\\Phi=\\{\\phi_1,\\,\\phi_2,\\,\\ldots\\}$ of points from a numerical method, where $\\phi_i=\\phi(q_i)$ and each $\\phi_i\\in\\mathbb{R}$. Let us consider the mean over the whole trajectory, which we can write with a bar over a function, as\n",
    "\n",
    "$$\\overline{f(\\phi_i)} = \\lim_{n\\to\\infty} \\frac1n \\sum_{i=1}^n f(\\phi_i) $$\n",
    "\n",
    "In practice we can estimate this value by considering a trajectory of finite length, and just averaging over that.\n",
    "\n",
    "The _autocorrelation function_ can be defined as \n",
    "\n",
    "$$\\text{autocorr}(k;\\,\\Phi) = \\frac{\\overline{(\\phi_i - \\mu)(\\phi_{i+k} - \\mu)} }{\\overline{(\\phi_i - \\mu)^2 }}$$\n",
    "\n",
    "where $k\\in\\mathbb{N}\\cup\\{0\\}$ is called the _lag_, and we have denoted $\\mu = \\overline{\\phi_i}$ as the average of $\\phi$ over the trajectory. As we don't have an infinite trajectory from our simulation in practice, we estimate the value as best as we can for each $k$, and at $k$ large this means that we don't have many points to average over. It should be clear that at $k=0$ this function will be $1$, but as $k$ gets large the lack of points to average will result in a noisier estimate of the $n\\to\\infty$ limiting case,\n",
    "\n",
    "The autocorrelation function shows how quickly the method decorrelates from the initial condition. Decorrelation is good, it means that as we move to a new point we _forget_ our initial conditions, which is tantamount to mixing faster.\n",
    "\n",
    "**[3] Create a function `acf` that takes in a trajectory $\\Phi$, and a max lag `kmax`, and outputs the autocorrelation function between $0$ and $k$ (so a vector of length `kmax`$+1$).**\n",
    "\n",
    "** Plot the autocorrelation functions for $\\phi(q)=y$ (i.e. the $y$ component) of trajectories generated using the Metropolis Random Walk scheme on the Rosenbrock example, using $h=0.25$, $0.5$, and $1.0$ using $100000$ steps, and for max lag 100 steps.**\n",
    "\n",
    "*(Don't forget to use axis labels and legends here and whenever you plot things to answer a handin question.)*\n",
    "\n",
    "You should see that the correlations decay as a function of number of steps $k$, with the largest $h$ (giving the smallest acceptance rate) most rapidly decorrelating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acf(Phi,kmax):\n",
    "    \n",
    "    # Your code here \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integrated autocorrelation time $\\tau$ for the function $\\phi(q)$ can give us a measure of how quickly things decorrelate. This is related to the infinite sum of the autocorrelation function, and is explicitly calculated as:\n",
    "\n",
    "$$\\tau_\\phi = 1 + 2 \\sum_{k=1}^\\infty \\text{autocorr}(k;\\, \\Phi)$$\n",
    "\n",
    "There is usually significant noise in the plot of the autocorrelation function, leading to a sum that grows as $k\\to\\infty$. In the limit of infinite sampling this noise dies away and the autocorrelation function tends to zero as $k$ increases. To overcome this noise in practice, a common technique is to zero values from the `acf` function for all $k$ beyond the first $k$ such that $\\text{autocorr}(k)<0.01$. This will ensure a more stable result (if a slightly less accurate one).\n",
    "\n",
    "**[4] Approximate $\\tau_\\phi$ for the three trajectories you ran in Q3.**\n",
    "\n",
    "*(Hint: You should make use of the approximation mentioned above, and will likely need to compute a longer acf than you did in Q3. If you wish, to get a more accurate answer you can try computing an acf from running longer trajectories.)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this $\\tau$ value to approximate the asymptotic variance that we expect to see in our computed average. We will denote $\\hat{\\phi}$ as our observed average of $\\phi$ after $N$ steps of the numerical scheme, so $\\hat{\\phi} = \\frac1N\\sum_{i=1}^N \\phi_i$. If we run lots of independent simulations, you can imagine that we will get slightly different answers every time we run, due to the random nature of the schemes. The variance of our estimate of the mean can be computed via the central limit theorem, which says that as the number of steps $N$ gets large\n",
    "\n",
    "$$ \\text{var}(\\hat{\\phi}) \\to \\frac{(\\text{Av}(\\phi^2)-\\text{Av}(\\phi)^2) \\tau_\\phi}{N} $$\n",
    "\n",
    "Recall that $\\text{Av}(\\bullet)$ denotes the true average with respect to $\\rho$. The value $\\text{var}(\\hat{\\phi})$ can be thought of as the _error_ we expect in our computed average: the expected variation in our estimate. An efficient method will have a small variance.\n",
    "\n",
    "**[5] Approximate $\\text{var}(\\hat{\\phi})$ when $N=10000$ using Metropolis Random Walk with $h=0.5$, by computing the variance of the computed $\\hat{\\phi}$ over fifty independent experiments, using $\\phi(q)=y$ as before.**\n",
    "\n",
    "*(You should find a value somewhere between 0.005 and 0.01)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this variance to the theoretical result, as long as we have the exact average value. \n",
    "In this example we can compute the exact averages to arbitrary accuracy via a double-Riemann Sum, i.e. recall that we can write\n",
    "\n",
    "$$ \\int_A^B f(x) \\text{d}x \\approx \\sum_{i=1}^N f(x_i) \\Delta x$$\n",
    "\n",
    "for a small enough $\\Delta x$ and a choice of points $x_i\\in[A,B]$. This will  approximate the (double) integral as a discrete sum in space.\n",
    "\n",
    "**[6] Use a Riemann Sum (or other integration routine) to show that $(\\text{Av}(\\phi^2)-\\text{Av}(\\phi)^2)\\approx0.66$ where recall**\n",
    "$$\\text{Av}(f(x,y)) := \\int_{-\\infty}^\\infty\\int_{-\\infty}^{\\infty} f(x,y) \\rho(x,y) \\text{d}x\\text{d}y,\\qquad  \\rho(x,y) = \\frac{\\exp(-U(x,y))}{\\int_{-\\infty}^\\infty \\exp(-U(x,y))\\text{d}x\\text{d}y}$$\n",
    "\n",
    "_(Hint: Though the integral is to $\\pm\\infty$, computing the integral in a box $x\\in[-4,6]$, $y\\in[-2,15]$ will suffice as $\\rho$ becomes extremely small outside of that region. Your code should be able to compute this quickly, without loops. )_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that indeed\n",
    "$$ \\text{var}(\\hat{\\phi}) \\approx \\frac{(\\text{Av}(\\phi^2)-\\text{Av}(\\phi)^2) \\tau_\\phi}{N} $$\n",
    "and so one way to reduce observed error (i.e. the variance) is to find methods that decorrelate quickly and minimize their integrated autocorrelation $\\tau$ parameter.\n",
    "\n",
    "Running for a larger $N$, or improving your estimation with more repeats, will improve this agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "As an alternative to the Metropolis random walk, we may consider using methods that use the force information to drive points in the correct direction. For that, we need a force routine:\n",
    "\n",
    "\n",
    "**[7] Define a `force` function that takes a $q$ as input and outputs $-\\nabla U(q)$ for the Rosenbrock given above.**\n",
    "\n",
    "*(You can check that your gradient is correct by approximating the limit in the definition of the gradient by a small number, i.e. use $f'(x)\\approx (f(x+h)-f(x))/h$ for a small value of $h$.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def force(q):\n",
    "    \n",
    "    # Your code here \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be looking at the _Hybrid/Hamiltonian Monte Carlo_ method (HMC).  A good definition of the algorithm, along with pseudocode, is given in \n",
    "\n",
    "_MCMC using Hamiltonian dynamics_, Radford Neal (2011)\n",
    "\n",
    "The whole article is relevant, but the algorithm itself can be found in Section 3.\n",
    "\n",
    "**[8] Look up and implement the HMC algorithm in a function below. It should take as input the current position $q$ and a stepsize $h$. It should output the new position $q$ and an integer ($1$ if new point was accepted, $0$ if new point was rejected).**\n",
    "\n",
    "**Run a simulation of 100000 steps at $h=0.2$ and plot the autocorrelation function of $\\phi(q)=y$.**\n",
    "\n",
    "Note that in their description, `epsilon` corresponds to $h$, while `rnorm` gives a normal random number and `runif` is a uniform random number. He also uses an $L$ parameter to define the number of steps to take, for this example you should set $L=1$.\n",
    "\n",
    "You can test your routine using the `run_simulation` function. You should see that with $h=0.2$ you get an acceptance rate around $0.65$ or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HMC(q0,h):\n",
    "    \n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "If you experiment with the schemes, you should find that as you vary $h$ you do not get a significant improvement on decorrelation rate compared to the Metropolis Random Walk scheme. This is because in 2D it is very easy to come up with acceptable moves by doing something 'dumb' like adding a random number in any direction. In higher dimensions, this becomes significantly more challenging.\n",
    "\n",
    "Now consider expanding our position space, so $[x,y,z_1,z_2,\\ldots,z_{100}]=q\\in\\mathbb{R}^{102}$. We have $\\rho(q) \\propto \\exp(-U(q))$ as usual, where\n",
    "\n",
    "$$ U(q) =  (1-x)^2 + 4( 2y-x^2)^2 + \\frac12 \\sum_{i=1}^{100} z_i^2$$\n",
    "\n",
    "It should be clear that all we have done is add 100 new $z$ variables. These $z$ variables are called _nuisance_ variables, as they do not interact with the $x$ and $y$. They do not affect the averages in the integrals either; if you write down the integrals then the $z$ variables can be integrated out and cancel. But they are useful for looking at how the numerical methods deal in a higher dimensional setting.\n",
    "\n",
    "**[9] In this problem, which method, HMC or Metropolis Random Walk, do you think is more efficient at computing $\\text{Av}(\\phi)$, where $\\phi(q)=y$?**\n",
    "\n",
    "** Justify your answer by using Figures and writing markup text below.**\n",
    "\n",
    "*(Only a general overview is necessary, no more than a few hundred words of markup. It is advisable to put the markup text first and refer to any Figures, and the code to generate them, underneath. Figures to support your argument should come from code run on this worksheet, do not import outside assets from e.g. other articles. You can answer this in many different ways, there is not necessarily one right answer.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Optional:**\n",
    "1. Try running the Metropolis and HMC schemes on the Tomatoes example from last week. Does HMC outperform Metropolis?\n",
    "2. Compare the HMC method to the Langevin schemes coded last week, which is more efficient?\n",
    "3. Can you relate the $\\gamma$ parameter in Langevin dynamics to the integrated autocorrelation time, for a 1D Normal distribution?\n",
    "4. For a 1D Normal distribution, what is the optimal acceptance ratio for Metropolis and HMC?\n",
    "5. Implement and test the MALA scheme given in the lecture slides. Does it offer an improvement?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
