{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSCAM Week 3 exercises\n",
    "\n",
    "### Simulating Brownian dynamics\n",
    "\n",
    "Our task is to generate points according to the distribution\n",
    "$$\\rho_\\beta(q) = \\frac1{Z_\\beta} \\exp(-\\beta U(q)),\\qquad Z_\\beta=\\int_{-\\infty}^\\infty \\exp(-\\beta U(q) )\\,\\rm{d}q.$$\n",
    "This is known as the _sampling problem_, and can be found in a vast number of applications in computational applied mathematics. We can think of $\\rho_\\beta(q)$ as a probability distribution, with $\\int_{-\\infty}^\\infty \\rho_\\beta(q)\\,\\rm{d} q = 1.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, if $U(q)$ is a complicated non-linear function then this distribution is difficult to visualise, particularly if $q$ is a high-dimensional vector.\n",
    "\n",
    "As $\\rho_\\beta(q)$ is a probability distribution, we can think of averages (or expectations) of a function with respect to this distribution. We often write this as:\n",
    "\n",
    "$$ ``\\text{The average of a function }f(q)'' = {\\text{Av}}_{\\rho_\\beta}(f(q)) = \\int_{-\\infty}^\\infty f(q) \\rho_\\beta(q) \\, \\rm{d} q.$$\n",
    "\n",
    "This integral is typically too difficult to compute by hand, or approximate using something like Simpson's rule. We can get around this by computing a trajectory that solves the stochastic differential equation (SDE)\n",
    "$$ \\dot{q} = - \\nabla U(q) + \\sqrt{2/\\beta} \\dot{W},$$\n",
    "where $W(t)$ is a Wiener process. This SDE is known as Brownian dynamics, relating to Brownian motion. Solutions to this dynamics $q(t)$ have the useful property that:\n",
    "$$ \\lim_{t\\to\\infty} \\mathbf{E}\\left[ f(q(t)) \\right] = \\text{Av}_{\\rho_\\beta}(f(q)) $$\n",
    "<!--- $$ \\lim_{N\\to\\infty}\\frac1N \\sum_{n=1}^N f(q(nh)) = \\langle f(q) \\rangle$$ -->\n",
    "So in order to compute the target average, we solve the SDE to find trajectories $q(t)$ and take averages over that at a large time $t$.\n",
    "\n",
    "\n",
    "We shall look at one particular case where we can treat things analytically : the normal (or Gaussian) distribution; in one dimension, a normal distribution with unit variance and zero mean corresponds to $U(q) = q^2/2$.\n",
    "\n",
    "<!---In this case, we can compute explicitly\n",
    "$$Z_\\beta = \\int_{-\\infty}^\\infty \\exp(-\\beta q^2/2 )\\,\\rm{d}q = \\sqrt{\\frac{2\\pi}\\beta}$$-->\n",
    "\n",
    "\n",
    "**[1] : If **$U(q)=q^2/2$** with **$q\\in\\mathbb{R}$**, compute analytically the values of** $\\text{Av}_{\\rho_\\beta}(q)$, $\\text{Av}_{\\rho_\\beta}(q^2)$ **and** $\\text{Av}_{\\rho_\\beta}(q^3)$ **for** $\\beta=1$,** and define these values as constants below.**\n",
    "\n",
    "(_Hint: Try integrating by parts._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the averages below:\n",
    "exact_average_q  = 0 #?\n",
    "exact_average_q2 = 1 #?\n",
    "exact_average_q3 = 0 #?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For general $U(q)$ we can't solve to find $q(t)$ exactly, so we have to use an approximation scheme. We will first consider the  Euler-Maruyama scheme\n",
    "\n",
    "$$ \\left| \\begin{array}{rcl} \n",
    "R(t+h)&\\leftarrow&\\text{Normal}(0,1)\\\\\n",
    "\\qquad q(t+h)&\\leftarrow&q(t) + h\\, F(q(t)) + \\sqrt{2 h / \\beta} R(t+h)\n",
    "\\end{array}\\right.$$\n",
    "where $F(q)$ is the usual force $F(q)=-\\nabla U(q)$, and the 'Normal' function gives us a normal random number, here with mean 0 and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EulerMaruyama(q, R, h, force, beta): \n",
    "    \n",
    "    # Get a normal random vector R, of size len(q)\n",
    "    # By default, this gives us something with mean 0 and variance 1\n",
    "    R_t = np.random.randn(len(q))\n",
    "    \n",
    "    #Update the position\n",
    "    q_t = q + h*force(q) + np.sqrt(2*h/beta) * R_t\n",
    "    \n",
    "    return q_t, R_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `force` keyword is a function that returns $-\\nabla U(q)$. For $U(q)=q^2/2$, this force is just $F(q)=-q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianForce(q):\n",
    "    \n",
    "    # Force = -d_dq U(q)\n",
    "    # If U(q) = q^2/2, then...\n",
    "    f = -q \n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `run_simulation` function from last week, now modified to include the `beta` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation( q0, Nsteps, h, beta, step_function, force_function):\n",
    "    \n",
    "    q_traj = [] \n",
    "    t_traj = []\n",
    "\n",
    "    q = np.copy(q0)\n",
    "    R = 0\n",
    "    t = 0 \n",
    "\n",
    "    for n in range(Nsteps):\n",
    "        q,R = step_function(q, R, h, force_function, beta)\n",
    "        t = t + h \n",
    "\n",
    "        q_traj += [q]  \n",
    "        t_traj += [t] \n",
    "\n",
    "    q_traj = np.array(q_traj) \n",
    "    t_traj = np.array(t_traj) \n",
    "\n",
    "    return q_traj, t_traj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this outputs the trajectory of the position $q(t)$ as well as the trajectory of $t$. We can use this routine to evolve forwards in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = np.zeros(1)  # One single run, starting at q=0\n",
    "h = 0.05 # Stepsize\n",
    "T = 12  # Total time\n",
    "Nsteps = int(np.round(T/h))  #Number of steps\n",
    "q_traj, t_traj = run_simulation( q0 , Nsteps, h, 1,  EulerMaruyama, GaussianForce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_traj, q_traj)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('q(t)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can run this multiple times and it will give different-looking plots, as we're using random numbers.\n",
    "\n",
    "Though the plot will look different, the statistics of the trajectories will be the same. We can look at these properties by considering an _ensemble_ of particles. This just means we run multiple copies in parallel, and average over them to get statistics.\n",
    "Let's try plotting an ensemble of 100 trajectories, each trajectory is known as a _walker_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = np.zeros(100) # 100 walkers, starting at q=0.\n",
    "h = 0.05 \n",
    "T = 12\n",
    "Nsteps = int(T/h)\n",
    "q_traj, t_traj = run_simulation( q0 , Nsteps, h, 1,  EulerMaruyama, GaussianForce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_traj, q_traj)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('q(t)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit of a mess to look at.\n",
    "\n",
    "Much more sensible is to look at how expectation values change over time. We can do this by taking the mean over the ensemble, and in the limit of infinite walkers this will tend to the correct expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean of f(q) = q^2 over the ensemble:\n",
    "ensemble_mean = np.mean( q_traj**2,1)\n",
    "\n",
    "# Show how this ensemble mean changes with time\n",
    "plt.plot( t_traj, ensemble_mean, label='Approximated using 100 walkers' )\n",
    "\n",
    "# We also plot the value of Av(q^2)\n",
    "plt.plot( [0,12], [exact_average_q2,exact_average_q2],'--k',label='Exact value of Av($q^2)$')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time $t$')\n",
    "plt.ylabel('Approximate $E[q(t)^2]$') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have approximated $\\mathbf{E}[ q(t)^2]$ using our ensemble of 100 walkers. Like any approximated expectation, it gets better if we average over more datapoints.\n",
    "\n",
    "**[2] Plot the value of **$\\mathbf{E}[q(t)^2]$** for **$h=0.05$** and **$T=12$**, approximated using 100, 1000 and 10000 walkers.**\n",
    "\n",
    "_(Plot the three curves on the same axis. Axis labels and a legend will be expected to be included.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.05\n",
    "T = 12\n",
    "Nsteps = int(np.round(T/h))\n",
    "\n",
    "q0 = np.zeros(100) # 100 walkers, starting at q=0.\n",
    "q0_1000 = np.zeros(1000)\n",
    "q0_10000 = np.zeros(10000)\n",
    "\n",
    "h = 0.05 \n",
    "T = 12\n",
    "Nsteps = int(T/h)\n",
    "\n",
    "q_traj, t_traj = run_simulation( q0 , Nsteps, h, 1,  EulerMaruyama, GaussianForce)\n",
    "q_traj1000, t_traj1000 = run_simulation( q0_1000 , Nsteps, h, 1,  EulerMaruyama, GaussianForce)\n",
    "q_traj10000, t_traj10000 = run_simulation( q0_10000 , Nsteps, h, 1,  EulerMaruyama, GaussianForce)\n",
    "\n",
    "# Take the mean of f(q) = q^2 over the ensemble:\n",
    "ensemble_mean = np.mean( q_traj**2,1)\n",
    "ensemble_mean1000 = np.mean( q_traj1000**2,1)\n",
    "ensemble_mean10000 = np.mean( q_traj10000**2,1)\n",
    "\n",
    "\n",
    "# Show how this ensemble mean changes with time\n",
    "plt.plot( t_traj, ensemble_mean, label='Approximated using 100 walkers' )\n",
    "plt.plot( t_traj, ensemble_mean1000, label='Approximated using 1000 walkers' )\n",
    "plt.plot( t_traj, ensemble_mean10000, label='Approximated using 10000 walkers' )\n",
    "\n",
    "# We also plot the value of Av(q^2)\n",
    "plt.plot( [0,12], [exact_average_q2,exact_average_q2],'--k',label='Exact value of Av($q^2)$')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time $t$')\n",
    "plt.ylabel('Approximate $E[q(t)^2]$') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the expectation 'settles down' as you average over more walkers. Also as $t\\to\\infty$ we have $\\mathbf{E}[q(t)^2]\\to\\rm{Av}(q^2)$, with the error becoming very small even at modest values of $t$.\n",
    "\n",
    "As well as looking at the evolution of expectations, we can also look at the evolution of the distribution of the points themselves. At large times $t$, the distribution of $q(t)$ should tend to the target distribution $\\rho_\\beta(q)$. We can check that in our simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a simulation with 10000 walkers\n",
    "q0 = np.zeros(10000)\n",
    "h = 0.05\n",
    "T = 5\n",
    "Nsteps = int(T/h)\n",
    "q_traj, t_traj = run_simulation( q0 , Nsteps, h, 1,  EulerMaruyama, GaussianForce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot a histogram of the ensemble at the end point of our trajectory, and compare it to the target distribution. Using `density=True` will normalize the histogram for us in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the distribution at the end point in time \n",
    "hgram,bins = np.histogram( q_traj[-1,:],range=[-4,4],bins=50,density=True)\n",
    "midx = (bins[1:]+bins[:-1])/2  # Get the midpoints of the bin edges\n",
    "plt.plot(midx, hgram, linewidth=3,label='Approx. distribution at $t=5$')\n",
    "\n",
    "Z_beta = np.sqrt(2*np.pi)  # We know this value for a Gaussian!\n",
    "rho_beta = np.exp(- midx**2 /2 ) / Z_beta\n",
    "plt.plot(midx,rho_beta,':k',linewidth=3,label='Target rho_beta')\n",
    "plt.legend()\n",
    "plt.xlabel('q')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see they line up really well! If you add more walkers then you should see the approximate histogram will get even smoother.\n",
    "\n",
    "This is what the distribution of ensemble points looks like at the end of the simulation, what about throughout the simulation?\n",
    "\n",
    "**[3] Plot the evolving distribution of **$q(t)$** at times **$t\\in\\{0.05,0.1,0.2,2\\}$**, from a simulation with **$h=0.05$** and 10000 walkers.**\n",
    "\n",
    "_(Plot the curves on the same axis. Axis labels and a legend will be expected to be included.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the distribution at different times\n",
    "hgram,bins = np.histogram( q_traj[0,:],range=[-4,4],bins=50,density=True )\n",
    "midx = (bins[1:]+bins[:-1])/2  # Get the midpoints of the bin edges\n",
    "plt.plot(midx, hgram, linewidth=3,label='Approx. distribution at $t=0.05$')\n",
    "\n",
    "# t=0.1\n",
    "hgram1,bins1 = np.histogram( q_traj[1,:],range=[-4,4],bins=50,density=True )\n",
    "midx1 = (bins1[1:]+bins1[:-1])/2  # Get the midpoints of the bin edges\n",
    "plt.plot(midx1, hgram1, linewidth=3,label='Approx. distribution at $t=0.1$')\n",
    "\n",
    "# t=0.2\n",
    "hgram2,bins2 = np.histogram( q_traj[3,:],range=[-4,4],bins=50,density=True )\n",
    "midx2 = (bins2[1:]+bins2[:-1])/2  # Get the midpoints of the bin edges\n",
    "plt.plot(midx2, hgram2, linewidth=3,label='Approx. distribution at $t=0.2$')\n",
    "\n",
    "# t=2\n",
    "hgram3,bins3 = np.histogram( q_traj[39,:],range=[-4,4],bins=50,density=True )\n",
    "midx3 = (bins3[1:]+bins3[:-1])/2  # Get the midpoints of the bin edges\n",
    "plt.plot(midx3, hgram3, linewidth=3,label='Approx. distribution at $t=2$')\n",
    "\n",
    "Z_beta = np.sqrt(2*np.pi)  # We know this value for a Gaussian!\n",
    "rho_beta = np.exp(- midx**2 /2 ) / Z_beta\n",
    "plt.plot(midx,rho_beta,':k',linewidth=3,label='Target rho_beta')\n",
    "plt.legend(loc=2, prop={'size': 8})\n",
    "plt.xlabel('q')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that at short times the distribution of $q(t)$ is nowhere near the target distribution, so the target averages will be poor.\n",
    "\n",
    "But if all we care about is the expectation at large $t$, why not increase the stepsize $h$ so we do less work (i.e. `Nsteps` is smaller)?\n",
    "\n",
    "We will denote by $\\hat{q}_h(t)$ a trajectory generated using our numerical method using a stepsize $h$, at time $t$. We can ask how $\\mathbf{E}[\\hat{q}_h(t)]$ compared to the true evolution $\\mathbf{E}[q(t)]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4] Plot the evolution of **$\\mathbf{E}[\\hat{q}_h(t)^2]$**, computed from simulations with **$T=4$** and using stepsizes **$h\\in\\{0.05,0.1,0.2,0.4,0.8\\}$.\n",
    "\n",
    "_(Plot the curves on the same axis. Axis labels and a legend will be expected to be included. You should use enough walkers so that the curves do not appear noisy: 100000 should be fine.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = np.zeros(10000)\n",
    "h = [0.05,0.1,0.2,0.4,0.8]\n",
    "T = 4\n",
    "\n",
    "for step in h:\n",
    "    Nsteps = int(T/step)\n",
    "    q_traj, t_traj = run_simulation( q0 , Nsteps, step, 1,  EulerMaruyama, GaussianForce)\n",
    "    ensemble_mean = np.mean( q_traj**2,1)\n",
    "    legend = 'Approximated using h =' + str(step)\n",
    "    plt.plot( t_traj, ensemble_mean, label= legend)\n",
    "\n",
    "plt.plot( [0,4], [exact_average_q2,exact_average_q2],'--k',label='Exact value of Av($q^2)$')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time $t$')\n",
    "plt.ylabel('Approximate $E[q(t)^2]$') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the results for larger $h$ give a bigger error, although we get these results faster as we have to do fewer steps. (Optional: What happens if you try larger stepsizes? Does it always converge?) The Euler method is known to be first order accurate for long-time sampling, which means that the error \n",
    "\n",
    "$$ \\left|\\mathbf{E}[ f(q(T)) ] - \\rm{Av}_{\\rho_\\beta}(f(q))\\right| = O(h) $$\n",
    "\n",
    "for large $T$. We have seen in the plot for Q4 that at $T=4$ we have essentially converged to the correct average, so we can look at the error we make at that time.\n",
    "\n",
    "**[5] Using **$10^6$** walkers, with **$T=4$**, compute the value of **$\\mathbf{E}[\\hat{q}_h(T)^2]$** for **$h\\in\\{0.025,0.05,0.1,0.2\\}$**, and show that the error (defined above) changes linearly with stepsize, using a `loglog` plot.**\n",
    "\n",
    "_(Axis labels and a legend will be expected to be included. You should include an $O(h)$ guideline to show that the error follows that behaviour: i.e. add `plt.plot([0.025,0.05,0.1,0.2],[0.025,0.05,0.1,0.2])`)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = np.zeros(int(1e6))\n",
    "h = [0.025,0.05,0.1,0.2]\n",
    "T = 4\n",
    "error = np.zeros(len(h))\n",
    "itr = 0\n",
    "for step in h:\n",
    "    Nsteps = int(T/step)\n",
    "    q_traj, t_traj = run_simulation( q0 , Nsteps, step, 1,  EulerMaruyama, GaussianForce)\n",
    "    ensemble_mean = np.mean( q_traj[-1,:]**2)\n",
    "    error[itr] = abs(ensemble_mean - exact_average_q2) \n",
    "    itr = itr+1\n",
    "    \n",
    "plt.loglog( h,error,label ='Numerical Error')\n",
    "\n",
    "plt.loglog([0.025,0.05,0.1,0.2],[0.025,0.05,0.1,0.2],label='O(h)')\n",
    "plt.legend()\n",
    "plt.xlabel('Step size (h)')\n",
    "plt.ylabel('Error') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as interesting as the long-time distribution is the evolution of the average at short time. This is the so-called _weak_ evolution of the system, with the weak error at time $t$ defined as \n",
    "\n",
    "$$ {\\text{Weak error at time }}t = \\left| \\mathbf{E}[ f(\\hat{q}_h(t))] - \\mathbf{E}[ f(q(t))]\\right|$$\n",
    "\n",
    "If $U(q)=q^2/2$, then you are given that $\\mathbf{E}[q(t)^2] = 1 - \\exp(-2t)$. _(Optional: This can be shown via Ito's lemma.)_\n",
    "\n",
    "**[6] Using **$h\\in\\{0.025,0.05,0.1,0.2\\}$**, show that the weak error at $t=0.6$ changes linearly with stepsize, using a `loglog` plot. In other words, show that Euler-Maruyama is a weak first order method.**\n",
    "\n",
    "_(Axis labels and a legend will be expected to be included. You should include an $O(h)$ guideline to show that the error follows that behaviour: i.e. add `plt.plot([0.025,0.05,0.1,0.2],[0.025,0.05,0.1,0.2])`)_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = np.zeros(int(1e6))\n",
    "h = [0.025,0.05,0.1,0.2]\n",
    "T = 0.61 #to include 0.6 at the step size 0.2\n",
    "error = np.zeros(len(h))\n",
    "itr = 0\n",
    "for step in h:\n",
    "    Nsteps = int(T/step)\n",
    "    q_traj, t_traj = run_simulation( q0 , Nsteps, step, 1,  EulerMaruyama, GaussianForce)\n",
    "    ensemble_mean = np.mean( q_traj[-1,:]**2)\n",
    "    error[itr] = abs(ensemble_mean - (1-np.exp(-2*0.6)))\n",
    "    itr = itr+1\n",
    "    \n",
    "plt.loglog( h,error,label ='Weak Error')\n",
    "\n",
    "plt.loglog([0.025,0.05,0.1,0.2],[0.025,0.05,0.1,0.2],label='O(h)')\n",
    "plt.legend()\n",
    "plt.xlabel('Step size (h)')\n",
    "plt.ylabel('Error') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at using other numerical methods instead of Euler-Maruyama. One example is the LM method:\n",
    "\n",
    "$$ \\left| \\begin{array}{rcl} \n",
    "R(t+h)&\\leftarrow&\\text{Normal}(0,1)\\\\\n",
    "\\qquad q(t+h)&\\leftarrow&q(t) + h\\, F(q(t)) + \\frac12 \\sqrt{2 h / \\beta}\\left( R(t) + R(t+h) \\right)\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "Note that this still only requires one evaluation of $F$ per step, with $F$ usually being the computationally expensive part.\n",
    "\n",
    "**[7] Code a version of the LM method below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LM(q, R, h, force, beta): \n",
    "    \n",
    "    # Random R generation    \n",
    "    R_t = np.random.randn(len(q))\n",
    "    \n",
    "    #Update the position\n",
    "    q_t = q + h*force(q) + np.sqrt(2*h/beta) * (R+R_t)/2\n",
    "    \n",
    "    return q_t, R_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This LM function can be swapped in place of the EulerMaruyama function in `run_simulation` in order to be used.\n",
    "\n",
    "**[8] Repeat Q5 and Q6 using the LM method instead of Euler-Maruyama.**\n",
    "\n",
    "You should see that at small times (i.e. the results of Q6) you still have a first-order error, but at long times (i.e. results from Q5) there is no discernable trend. Conceptually it may help to plot the evolution of the expectation for the LM method, as you did in Q4.\n",
    "\n",
    "What is actually happening is that the LM scheme is _exact_ for Gaussians in the long time, but only first order in the short time. This is a special case for Gaussian distributions - for more general $U(q)$ functions, LM is $O(h^2)$ in long time, and $O(h)$ in short time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating Q4 to see the evolution for the LM Method\n",
    "q0 = np.zeros(10000)\n",
    "h = [0.05,0.1,0.2,0.4,0.8]\n",
    "T = 20\n",
    "\n",
    "for step in h:\n",
    "    Nsteps = int(T/step)\n",
    "    q_traj, t_traj = run_simulation( q0 , Nsteps, step, 1,  LM, GaussianForce)\n",
    "    ensemble_mean = np.mean( q_traj**2,1)\n",
    "    legend = 'Approximated using h =' + str(step)\n",
    "    plt.plot( t_traj, ensemble_mean, label= legend)\n",
    "\n",
    "plt.plot( [0,T], [exact_average_q2,exact_average_q2],'--k',label='Exact value of Av($q^2)$')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time $t$')\n",
    "plt.ylabel('Approximate $E[q(t)^2]$') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating Q5 for LM Method\n",
    "q0 = np.zeros(int(1e6))\n",
    "h = [0.025,0.05,0.1,0.2]\n",
    "T = 4\n",
    "error = np.zeros(len(h))\n",
    "itr = 0\n",
    "for step in h:\n",
    "    Nsteps = int(T/step)\n",
    "    q_traj, t_traj = run_simulation( q0 , Nsteps, step, 1,  LM, GaussianForce)\n",
    "    ensemble_mean = np.mean( q_traj[-1:]**2)\n",
    "    error[itr] = abs(ensemble_mean - exact_average_q2) \n",
    "    itr = itr+1\n",
    "\n",
    "plt.loglog( h,error,label ='Numerical Error')\n",
    "\n",
    "plt.loglog([0.025,0.05,0.1,0.2],[0.025,0.05,0.1,0.2],label='O(h)')\n",
    "plt.legend()\n",
    "plt.xlabel('Step size (h)')\n",
    "plt.ylabel('Error') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating Q6 for LM method\n",
    "q0 = np.zeros(int(1e6))\n",
    "h = [0.025,0.05,0.1,0.2]\n",
    "T = 0.61 #to include 0.6 at the step size 0.2\n",
    "error = np.zeros(len(h))\n",
    "itr = 0\n",
    "for step in h:\n",
    "    Nsteps = int(T/step)\n",
    "    q_traj, t_traj = run_simulation( q0 , Nsteps, step, 1,  LM, GaussianForce)\n",
    "    ensemble_mean = np.mean( q_traj[-1,:]**2)\n",
    "    error[itr] = abs(ensemble_mean - (1-np.exp(-2*0.6)))\n",
    "    itr = itr+1\n",
    "    \n",
    "plt.loglog( h,error,label ='Weak Error')\n",
    "\n",
    "plt.loglog([0.025,0.05,0.1,0.2],[0.025,0.05,0.1,0.2],label='O(h)')\n",
    "plt.legend()\n",
    "plt.xlabel('Step size (h)')\n",
    "plt.ylabel('Error') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### The double-well example\n",
    "\n",
    "The normal distribution is an important test case, but we can apply out schemes to more complicated potential energy functions where we cannot solve to find things exactly.\n",
    "\n",
    "We shall look at the _double-well_ potential energy, defined here as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$U(q) = 2(q^2-1)^2$$\n",
    "\n",
    "Note that this has two minima, at $q=\\pm1$. We can plot the form of the distribution by plotting $\\exp(-\\beta U(q))$. Note that this isn't the distribution itself, as we haven't normalised it by $Z_\\beta$. In fact, for an example even as simple as this, we cannot exactly evaluate $Z_\\beta$ !."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1 \n",
    "q_span = np.linspace(-3,3,100)\n",
    "Uq = 2*(q_span**2-1)**2\n",
    "rho = np.exp(- beta * Uq ) \n",
    "plt.plot( q_span, rho)\n",
    "plt.xlabel('q')\n",
    "plt.title('$\\exp(-U(q))$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the distribution has two large clusters centred around the minima. We can simulate this using the LM method as before.\n",
    "\n",
    "**[9] Code the force routine for the double-well potential below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoubleWellForce(q):\n",
    "    \n",
    "    # U(q) = 2(q^2-1)^2\n",
    "    \n",
    "    f = -8*q*(q**2-1) # [!]\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check here\n",
    "q0=1.1\n",
    "print( DoubleWellForce(q0) )\n",
    "# Should output -1.848  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialise 10 walkers at $q=1.1$ and see how they evolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = 1.1*np.ones(10)  # Initialise 10 walkers around 1.1\n",
    "h = 0.05\n",
    "T = 10\n",
    "Nsteps = int(np.round(T/h))\n",
    "q_traj, t_traj = run_simulation( q0 , Nsteps, h, 1,  LM, DoubleWellForce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( t_traj, q_traj )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that most of the walkers stay close to $q=1$, and by the end of the simulation some walkers have jumped over to the other well. This is an example of how barriers, like the one at $q=0$, can prevent a quick exploration of the surface. You can imagine in high-dimensions, for complicated $U(q)$, it can take a long time to pass over barriers you don't even know exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[10] Like in Q4, plot the evolving distribution of **$q(t)$** in this example at times **$t\\in\\{0.1,0.5,2.5,10\\}$**, from a simulation with **$h=0.05$** and 10000 walkers.**\n",
    "\n",
    "_(Plot the curves on the same axis. Axis labels and a legend will be expected to be included.)_\n",
    "\n",
    "You should see the distribution slowly spill into the other well. (Optional: Animate the evolution of the distribution).\n",
    "\n",
    "You can plot the distribution by making a histogram with the `density=True` flag, like in Q4. Parameters `range=[-2.5,2.5]` and `bins=50` give good results. You may wish to use more walkers to get a smoother histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = 1.1*np.ones(10000)  # Initialise 10000 walkers around 1.1\n",
    "h = 0.01\n",
    "T = 10\n",
    "\n",
    "Nsteps = int(T/h)\n",
    "q_traj, t_traj = run_simulation( q0 , Nsteps, h, 1,  LM, DoubleWellForce)\n",
    "\n",
    "t = [0.1,0.5,2.5,10]\n",
    "\n",
    "itr = 0\n",
    "for eacht in t:\n",
    "    hgram,bins = np.histogram( q_traj[int(eacht/h)-1,:],range=[-2.5,2.5],bins=50,density=True )\n",
    "    midx= (bins[1:]+bins[:-1])/2  # Get the midpoints of the bin edges\n",
    "    plt.plot(midx, hgram, linewidth=3,label='Approx. distribution at $t=$'+ str(eacht))\n",
    "    itr = itr+1\n",
    "\n",
    "plt.legend(loc=2, prop={'size': 8})\n",
    "plt.xlabel('q')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation Trial\n",
    "# Import Libs\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib inline\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Function Definition\n",
    "def animate_trajectory(q_traj,t_traj,skip=10,time_interval=100,colors='rgb'):\n",
    "    # There are lots of ways to animate things in Python. This is \n",
    "    # a simple way of doing it.\n",
    "    #\n",
    "    # skip :: How many frames to skip\n",
    "    # time_interval :: The time in ms between frames\n",
    "    # colors :: A list of colours that we use for the particles\n",
    "    \n",
    "    frames = []   \n",
    "    Nframes = int(len(q_traj)/skip) \n",
    "    \n",
    "    for n in range(Nframes): \n",
    "        qq = q_traj[n*skip,:] \n",
    "        frame = plt.scatter( t_traj,qq,s=10)    \n",
    "\n",
    "        frames.append( (frame,) )\n",
    "    im_ani = animation.ArtistAnimation(plt.gcf(),frames,interval=time_interval);\n",
    "    plt.close()\n",
    "    return im_ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = 1.1*np.ones(10000)  # Initialise 10000 walkers around 1.1\n",
    "h = 0.01\n",
    "T = 15\n",
    "\n",
    "Nsteps = int(T/h)\n",
    "q_traj, t_traj = run_simulation( q0 , Nsteps, h, 1,  LM, DoubleWellForce)\n",
    "\n",
    "t = np.linspace(0.1,15,200)\n",
    "\n",
    "hgram = np.zeros((len(t),50))\n",
    "midx = np.zeros((len(t),50))\n",
    "itr=0\n",
    "for eacht in t:\n",
    "    hgram[itr,:],bins = np.histogram( q_traj[int(eacht/h)-1,:],range=[-2.5,2.5],bins=50,density=True )\n",
    "    midx[itr,:]= (bins[1:]+bins[:-1])/2  # Get the midpoints of the bin edges\n",
    "    itr=itr+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the animation\n",
    "ani = animate_trajectory(hgram,midx[1])\n",
    "HTML(ani.to_jshtml())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:**\n",
    "1. For the Gaussian example, at what stepsize $h$ does the average no longer converge as $T$ increases? Is this also true in the LM method? You may wish to show this analytically.\n",
    "2. Compute the average time for a barrier crossing (i.e. a sign flip) in the double-well example. What happens if you increase/decrease the beta parameter? Find a relation between the crossing time as a function of beta.\n",
    "3. Run Q5/6/8 for the double-well example to show the error relation as a function of stepsize.\n",
    "4. Run a simulation using the 2D spring potential that we used in week 1, and plot a 2D histogram of the evolving distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
