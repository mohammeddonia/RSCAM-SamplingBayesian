{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSCAM Week 4 exercises\n",
    "\n",
    "[*Corrected version  February 11th*]\n",
    "\n",
    "**[0] Please rename your submitted file in the format `firstname_lastname.ipynb` using _File->Rename_.**\n",
    "\n",
    "\n",
    "### Simulating Langevin dynamics\n",
    "\n",
    "We assume we have some complicated potential energy function $U(q)$, and the target distribution $\\rho_\\beta(q)$ where \n",
    "$$\\rho_\\beta(q) = \\frac1{Z_\\beta} \\exp(-\\beta U(q)),\\qquad Z_\\beta=\\int_{-\\infty}^\\infty \\exp(-\\beta U(q) )\\,\\rm{d}q.$$\n",
    "Ultimately we want to compute averages with respect to this distribution, which we can write as an integral against $\\rho_\\beta(q)$.\n",
    "\n",
    "If we augment our distribution to include momentum $p$, then we can write \n",
    "$$\\rho_\\beta(q,p) = \\frac1{\\hat{Z}_\\beta} \\exp(-\\beta H(q,p) ),\\qquad \\hat{Z}_\\beta=\\int_{-\\infty}^\\infty \\exp(-\\beta H(q,p) )\\,\\rm{d}q\\rm{d}p,$$\n",
    "where $H$ is the familiar Hamiltonian, or total energy function, that we've seen before\n",
    "$$H(q,p) = \\frac12 \\|p\\|^2 + U(q)$$\n",
    "where we have assumed mass is 1 for all degrees of freedom.\n",
    "\n",
    "As $q$ and $p$ are independent, we can integrate out over the momentum variables and recover the correct sampling in $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The momentum variables are useful as they can allow a system to continue going in an unfavourable direction (i.e. up barriers) via inertia. This property is lacking in, for example, the Brownian dynamics example in Week 3.\n",
    "\n",
    "The equations of motion for Langevin dynamics are\n",
    "$${ \\begin{array}{rcl}\n",
    "\\dot{q} &=& p\\\\\n",
    "\\dot{p} &=& -\\nabla U(q) - \\gamma p + \\sqrt{2\\gamma /\\beta} \\dot{W}\\end{array}}$$\n",
    "where $W$ is a Wiener process, and $\\gamma>0$ is a _friction constant_ parameter that describes how rapidly the momentum is randomized ($\\gamma$ is like a rate). Note that is $\\gamma=0$ then we reduce to the constant energy dynamics that we looked at in Weeks 1 and 2. If $\\gamma$ is large, or infinite, then solutions will tend towards solutions to Brownian dynamics we generated in Week 3.\n",
    "\n",
    "We can rewrite this as the sum of three terms:\n",
    "\n",
    "\n",
    "$$\\left[ \\begin{array}{c} \\dot{q}\\\\ \\dot{p} \\end{array}\\right]\n",
    "= \\underbrace{\\left[\\begin{array}{c} p \\\\0 \\end{array}\\right]}_\\text{A}\n",
    "+ \\underbrace{\\left[\\begin{array}{c} 0\\\\F(q) \\end{array}\\right]}_\\text{B}\n",
    "+ \\underbrace{\\left[\\begin{array}{c} 0\\\\-\\gamma p + \\sqrt{2\\gamma/\\beta} \\dot{W} \\end{array}\\right]}_\\text{O}.$$\n",
    "As usual $F(q)=-\\nabla U(q)$ denotes the force.\n",
    "\n",
    "We can solve each of the pieces _exactly_ in an average sense:\n",
    "\n",
    "$$\\text{(A) dynamics step}$$\n",
    "$$ \\left| \\begin{array}{rcl} \n",
    "q(t+h)&\\leftarrow&q(t) + h  p(t) \\\\\n",
    "\\qquad p(t+h)&\\leftarrow&p(t)\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "$$\\text{(B) dynamics step}$$\n",
    "$$ \\left| \\begin{array}{rcl} \n",
    "q(t+h)&\\leftarrow&q(t) \\\\\n",
    "\\qquad p(t+h)&\\leftarrow&p(t) + h F(q(t))\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "$$\\text{(O) dynamics step}$$\n",
    "$$ \\left| \\begin{array}{rcl} \n",
    "q(t+h)&\\leftarrow&q(t) \\\\\n",
    "R(t+h)&\\leftarrow& \\text{Normal}(0,I)\\\\\n",
    "\\qquad p(t+h)&\\leftarrow&\\exp(-\\gamma h) p(t) + \\sqrt{1/\\beta}\\sqrt{1-\\exp(-2\\gamma h)} R(t+h)\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "The solution of the O step comes from the solution for the Ornstein-Uhlenbeck process, with $R(t)\\in\\mathbb{R}^N$ a vector of independent and identically distributed (i.i.d.) normal random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[1] Complete the `B_step` and `O_step` functions below to match the above algorithms.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_step( qp , h ):\n",
    "    q,p = qp\n",
    "    \n",
    "    q = q + h*p\n",
    "    p = p \n",
    "    \n",
    "    return [q,p]\n",
    "\n",
    "def B_step( qp , h, force ):\n",
    "    q,p = qp\n",
    "    \n",
    "    # Your code here \n",
    "    \n",
    "    return [q,p]\n",
    "\n",
    "def O_step( qp , h,gamma ):\n",
    "    q,p = qp\n",
    "    \n",
    "    # Your code here \n",
    "    \n",
    "    return [q,p]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define a method integrating Langevin Dynamics by performing these in sequence. For example, the _ABO_ method looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ld_ABO(q,p,h,gamma,force):\n",
    "    \n",
    "    # The algorithm \"ABO\" does A then B then O \n",
    "    \n",
    "    qp = [q,p]\n",
    "    \n",
    "    A = A_step(qp , h )\n",
    "    AB = B_step( A, h, force)\n",
    "    ABO = O_step( AB, h, gamma )\n",
    "    q,p = ABO\n",
    "    \n",
    "    return q , p "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed this function into our `run_simulation` function to try running it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation( q0,p0, Nsteps, h, gamma, step_function, force_function):\n",
    "    \n",
    "    q_traj = [np.copy(q0)] \n",
    "    p_traj = [np.copy(p0)]\n",
    "    t_traj = [0]\n",
    "\n",
    "    q = np.copy(q0) \n",
    "    p = np.copy(p0)\n",
    "    t = 0 \n",
    "\n",
    "    for n in range(Nsteps):\n",
    "        q,p = step_function(q, p, h, gamma, force_function)\n",
    "        t = t + h \n",
    "\n",
    "        q_traj += [q] \n",
    "        p_traj += [p]   \n",
    "        t_traj += [t] \n",
    "\n",
    "    q_traj = np.array(q_traj) \n",
    "    p_traj = np.array(p_traj) \n",
    "    t_traj = np.array(t_traj) \n",
    "\n",
    "    return q_traj, p_traj, t_traj\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than taking many walkers and looking at the evolving distributions, usually we will take one walker and run a long simulation. It can be shown that taking averages over this long simulation is equivalent to looking at the large time limit of lots of walkers, i.e. time averages along a trajectory will still equal averages with respect to the target distribution.\n",
    "\n",
    "For example, for a Gaussian we can look at the sampled distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_Gaussian(q):\n",
    "    # U(q) = q^2 /2\n",
    "    F = -q \n",
    "    return F\n",
    "\n",
    "# Initialize one walker from a Gaussian distribution\n",
    "\n",
    "q0 = np.random.randn(1)\n",
    "p0 = np.random.randn(1)\n",
    "\n",
    "Nsteps = 10000\n",
    "h = 0.5\n",
    "gamma = 1.0\n",
    "\n",
    "# Run one long trajectory of Nsteps, using the ABO scheme\n",
    "q_traj, p_traj, t_traj = run_simulation(q0, p0, Nsteps , h, gamma, ld_ABO, force_Gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then plot the resulting distributions in $q$ and $p$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15,4])\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot( t_traj, q_traj )\n",
    "plt.title('Trajectory of $q$')\n",
    "plt.ylabel('$q(t)$')\n",
    "plt.xlabel('Time $t$')\n",
    "\n",
    "plt.subplot(1,3,2) \n",
    "histogram,bins = np.histogram(q_traj,bins=50,range=[-3,3], density=True)\n",
    "midx = (bins[0:-1]+bins[1:])/2\n",
    "rho = np.exp(- (midx**2)/2)\n",
    "rho = rho / ( np.sum(rho) * (midx[1]-midx[0]) ) # Normalize rho by dividing by its approx. integral\n",
    "plt.plot(midx,histogram,label='Experiment')\n",
    "plt.plot(midx,rho,'--',label='Truth')\n",
    "plt.title('Distribution of $q$')\n",
    "plt.xlabel('$q$')\n",
    "plt.xlabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,3,3) \n",
    "histogram,bins = np.histogram(p_traj,bins=50,range=[-3,3], density=True)\n",
    "midx = (bins[0:-1]+bins[1:])/2\n",
    "rho = np.exp(- (midx**2)/2)\n",
    "rho = rho / ( np.sum(rho) * (midx[1]-midx[0]) ) # Normalize rho by dividing by its approx. integral\n",
    "plt.plot(midx,histogram,label='Experiment')\n",
    "plt.plot(midx,rho,'--',label='Truth')\n",
    "plt.title('Distribution of $p$')\n",
    "plt.xlabel('$p$')\n",
    "plt.xlabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2] Implement the _OBABO_ and _BAOAB_ step methods.**\n",
    "\n",
    "*(Recall that if a character appears twice in the string then that character's step is only for $h/2$, rather than $h$)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ld_OBABO(q,p,h,gamma,force):\n",
    "    \n",
    "    qp = [q,p]\n",
    "    \n",
    "    # Your code here \n",
    "    \n",
    "    return q , p \n",
    "\n",
    "def ld_BAOAB(q,p,h,gamma,force):\n",
    "    \n",
    "    qp = [q,p]\n",
    "    \n",
    "    # Your code here \n",
    "    \n",
    "    return q , p \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3] Compare the exact distribution to sampled distributions computed using _OBABO_ and _BAOAB_ by making a plot. Compute distributions in both $q$ and $p$, for the double-well potential energy function (below). Use a stepsize of $h=0.2$ with $\\gamma=1$, **\n",
    "$$U(q) = 2(q^2-1)^2$$\n",
    "\n",
    "*(Run using one walker for a long enough simulation so that your histogram looks smooth. Include axis labels, legends and titles where necessary.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_DoubleWell(q):\n",
    "    # U(q) = 2*(q^2-1)^2\n",
    "    \n",
    "    F = ? # Your code here \n",
    "    \n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we can estimate the rate at which the dynamics explores is by looking at the rate a trajectory spreads out in space. In this example, we shall look at barrier crossings, which we can count by looking at sign flips in $q$ (as the barrier's peak is at $q=0$). More sign flips in a trajectory mean more hops over the barrier between the left and right sides.\n",
    "\n",
    "**[4] Count the average number of sign flips in $q(t)$ for $t\\in[0,500]$ as a function of the choice of $\\gamma$, and plot the results for some choices of $\\gamma\\in[10^{-2},10^2]$. Initialize the starting position to be $q(0)=1$, and the initial momentum to be a normal random number $p(0)=\\text{Normal}(0,1)$.**\n",
    "\n",
    "_(Use BAOAB with $h=0.05$ and $10,000$ steps. To get good statistics (i.e. a smooth curve), you should average over lots of runs. Proper formatting of your Figure, i.e. Axis label etc., is required. )_\n",
    "\n",
    "_(NOTE: Loops are (usually) slow in Python. You should find a way to count the sign changes in a trajectory without looping over every element. Its advisable to try and do repeat experiments without looping as well. If successful, your code should run in minutes, instead of hours/years.)_\n",
    "\n",
    "You should find that the curve is hump-shaped when $\\gamma$ is on a log-scale, with $\\gamma$ too small or too large giving fewer sign changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Bayesian inference in data science\n",
    "\n",
    "The purpose of sampling is usually to calculate integrals with respect to a target distribution. We shall look at an application for this in Bayesian inference.\n",
    "\n",
    "We will look at one example for inferring a statistical property (the masses of tomatoes) from data via sampling.\n",
    "\n",
    "Let's assume there are three species of tomato: A, B and C. We will also assume that the masses of each species of tomato are normally distributed, with its mean mass being $\\mu_A,\\,\\mu_B$ and $\\mu_C$ grams, for each species respectively. We shall assume that the variance of the masses is 1 for each species.\n",
    "\n",
    "So if I take a single tomato with mass $y$, assuming it has an equal chance of being from species A, B or C, the _probability distribution_ $\\hat{\\rho}$ of $y$ is just the sum of the three Gaussian distributions:\n",
    "\n",
    "$$ \\hat{\\rho}(y\\,|\\,\\mu_A,\\mu_B,\\mu_C) \\propto \\exp({-(y-\\mu_A)^2/2}) + \\exp({-(y-\\mu_B)^2/2}) + \\exp({-(y-\\mu_C)^2/2})  $$\n",
    "\n",
    "If we know the values of $\\mu_A,\\,\\mu_B$ and $\\mu_C$ then we can compute the probability distribution for $y$. But what if we have the data, but don't know these three Gaussian centres? We can use Bayes theorem to do the reverse: find the probability distribution for the centres given the data.\n",
    "\n",
    "If we have a set of $N$ datapoints $Y = \\{y_i\\}$, then we can write the distribution for the centres as \n",
    "\n",
    "$$ \\rho(\\mu_A,\\mu_B,\\mu_C\\,|\\,Y) \\propto  \\rho_\\text{prior}(\\mu_A,\\mu_B,\\mu_C) \\prod_{i=1}^N \\hat{\\rho}(y_i\\,|\\,\\mu_A,\\mu_B,\\mu_C),$$\n",
    "where $\\rho_\\text{prior}$ is the _prior_ which regularizes the whole distribution, and $\\hat{\\rho}$ is the _likelihood_.\n",
    "\n",
    "Our goal in this section will be to sample $\\rho$ given some data $Y$. The distribution $\\rho$ looks very different to the previous distributions we have seen, as there is no potential energy $U(q)$. We will think of $q=[\\mu_A,\\mu_B,\\mu_C]$ and as we can write $\\rho(q) = \\exp(-\\log(\\rho(q)))$ we think of $U(q) = -\\log(\\rho(q))$.\n",
    "\n",
    "Thus we have \n",
    "\n",
    "$$ U(\\mu_A,\\mu_B,\\mu_C) = -\\log( \\rho_\\text{prior}(\\mu_A,\\mu_B,\\mu_C) ) - \\sum_{i=1}^N \\log( \\hat{\\rho}(y_i\\,|\\,\\mu_A,\\mu_B,\\mu_C) )$$\n",
    "\n",
    "\n",
    "In data science, $U$ is called the _log-posterior_ instead of the energy.\n",
    "\n",
    "Let's load in the data we will be using: the masses of 30 different tomatoes (of species A, B or C) picked fresh this morning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=np.load('tomatoes.npy') # Make sure you downloaded this file from Learn!\n",
    "print('Length of data: ' + str(len(Y)))\n",
    "print('Data:')\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can make a histogram to look at the distribution of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram,bins = np.histogram(Y,bins=10)\n",
    "mid_bins = (bins[:-1]+bins[1:])/2\n",
    "plt.bar(mid_bins,histogram,edgecolor='k')\n",
    "plt.xlabel('Mass (g)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Tomato data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall use a Gaussian prior with a large variance, just to keep us from sampling e.g. negative numbers. We will put the centre of the prior at 13.5, which is roughly the middle of the data.\n",
    "\n",
    "$$ \\rho_\\text{prior}(\\mu_A,\\mu_B,\\mu_C) \\propto \\exp({-(13.5-\\mu_A)^2/32}) \\exp({-(13.5-\\mu_B)^2/32}) \\exp({-(13.5-\\mu_B)^2/32}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5] Create a function that takes the value of the position $q$ and the data vector $y$, and outputs the value of $U(q)$.**\n",
    "\n",
    "*(NOTE: Try to avoid using a loop to sum over all of the data as it will be very slow, and instead work with vectors as much as possible. Don't forget U includes the prior and the likelihood.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U(q,y):\n",
    "    m1,m2,m3 = q \n",
    "    \n",
    "    # Your code here \n",
    "    \n",
    "    return V\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check:\n",
    "U( [12,13,14], Y) - U( [11,12,13], Y)\n",
    "# This should give \n",
    "# -23.67246392837439\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6] Create a function that takes the position $q$ and data $y$ and returns the force $\\nabla U(q)$. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def force(q,y):\n",
    "    m1,m2,m3 = q  \n",
    "    \n",
    "    return ?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force( [12,13,14], Y)\n",
    "# This should give force components as \n",
    "\n",
    "# -5.879399468415861,\n",
    "# 0.8083708852796383,\n",
    "# 17.568681144234255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[7] Use your force routine to generate samples from $\\rho$ by generating one long Langevin dynamics trajectory (i.e. over 10,000 steps).**\n",
    "\n",
    "**Plot histograms of the distributions of $\\mu_A,\\mu_B,\\mu_C$ on the same axis.**\n",
    "\n",
    "You should see that the values will like to settle between three defined values.\n",
    "\n",
    "*(You may use any Langevin scheme you wish to sample this problem. It is up to you to choose a sensible stepsize $h$ and friction $\\gamma$, and run for long enough to get smooth curves.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be hard to visualize the system when we have more than two dimensions, so people often use a pair of _collective variables_ and plot results in those instead.\n",
    "\n",
    "**[8] Plot a 2D histogram of your sampled trajectory in the $C_1$ and $C_2$ collective variable coordinates listed below.**\n",
    "\n",
    "$$ C_1 = \\mu_B - \\mu_A $$\n",
    "$$ C_2 = 2\\mu_C - \\mu_B - \\mu_A $$\n",
    "\n",
    "_(You may wish to plot the logarithm of the histogram value instead, which is more closely related to energy and makes a nice picture.)_\n",
    "\n",
    "You should see six defined regions where sampling is concentrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem has a symmetry in it, due to invariance under label permutation (this is called _fungification_). This means that the six sampled regions above are really the same region just with some projection mapping between them. We can remove this by collapsing the results onto one well.\n",
    "\n",
    "In general finding this projection can be very difficult, but here it is easy to accomplish. A transition between any of the six regions amounts to two of the $\\mu$ values swapping positions, so we can undo that by sorting our trajectory to maintain a size ordering.\n",
    "\n",
    "**[9] Use `np.sort` to create a copy of your trajectory with sorted components so that $\\mu_A<\\mu_B<\\mu_C$. Plot a new histogram of the distributions of the sorted $\\mu_A,\\mu_B,\\mu_C$ on the same axis, and print the means of each of the three coordinates.**\n",
    "\n",
    "You should see that each density is concentrated around just one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:**\n",
    "1. Use your A, B and O pieces to implement some higher-order Yoshida Langevin dynamics schemes.\n",
    "2. Demonstrate that the long-time sampling error for OBABO is second order for $q^2$, and BAOAB is second-order for $p^2$, in the Gaussian case.\n",
    "3. Compare stability for the BAOAB scheme in the case of the double-well potential as $\\gamma$ is increased.\n",
    "4. Modify the tomatoes example to include other parameters in the distribution, including component weights and variances. This is known as a Gaussian mixture model.\n",
    "5. Find the height of the barrier between two of the regions in the six-region histogram.\n",
    "6. Rerun Q4 for the data science example, counting the number of region changes instead of barrier hops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
